{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERSONALITY CLASSIFICATION USING NLP\n",
    "In this project we use the (MBTI) Myers-Briggs Personality Type Dataset from kaggle (https://www.kaggle.com/datasnaek/mbti-type) to predict and classify the personality type of a new user based on a collection of the user's prevous text messages. We will be using Linear Support Vector Classification method from sklearn.svm for this model. This method is ideal because the large data set(we have 8674 samples) and the need for a small runtime.\n",
    "\n",
    "The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:\n",
    "\n",
    "1. Introversion (I) – Extroversion (E)\n",
    "2. Intuition (N) – Sensing (S)\n",
    "3. Thinking (T) – Feeling (F)\n",
    "4. Judging (J) – Perceiving (P)\n",
    "\n",
    "So for example, someone who prefers introversion, intuition, thinking and perceiving would be labelled an INTP in the MBTI system, and there are lots of personality based components that would model or describe this person’s preferences or behaviour based on the label.\n",
    "\n",
    "For more information about data set please click the link above.\n",
    "\n",
    "The project will be split into two parts:\n",
    "* Part 1: Using nlp techniques to clean and organzie the data using nltk library.\n",
    "* Part 2: Using TF-IDF and LinearSVC to perform ML and obtain predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8675 entries, 0 to 8674\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    8675 non-null   object\n",
      " 1   posts   8675 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 135.7+ KB\n",
      "None\n",
      "type     0\n",
      "posts    0\n",
      "dtype: int64\n",
      "   type                                              posts\n",
      "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "1  ENTP  'I'm finding the lack of me in these posts ver...\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# import data and explore\n",
    "data = pd.read_csv(\"/Users/krishnan/desktop/mbti_1.csv\")\n",
    "# explore data\n",
    "print(data.info())\n",
    "print(data.isnull().sum())\n",
    "print(data.head(2))\n",
    "\n",
    "# we dont have any null values but we have links which we will remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will now create some functions to perform nlp techniques and cleaning on the 'posts' column\n",
    "\n",
    "# this function replaces words joined by '|||' , ',' , '.' , '?' , '!' and replaces any \n",
    "# symbols and characters mid word with a blank space.\n",
    "def cleaning(sentence):\n",
    "    new = sentence.replace('|||', ' ')\n",
    "    new = new.replace('.', ' ')\n",
    "    new = new.replace(',', ' ')\n",
    "    new = new.replace('?',' ')\n",
    "    new = new.replace('!', ' ')\n",
    "    new = re.sub('[^a-zA-Z]', ' ', new)\n",
    "    new = new.lower()\n",
    "    return new\n",
    "\n",
    "# this function tokenizes the sentences in the 'posts' column \n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text,'english')\n",
    "\n",
    "# for this project we will use lemmatization instead of stemming because of some inaccurate stemming results\n",
    "lemma_inst = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize all the tokens in a list of tokens and return a new list of tokens\n",
    "def lemma(list):\n",
    "    new_list=[]\n",
    "    for i in list:\n",
    "        new_list.append(lemma_inst.lemmatize(i))\n",
    "    return new_list\n",
    "\n",
    "# this function will remove all the stop words that are defined by nltk and returns a new list\n",
    "def stopwordremoval(list):\n",
    "    new_list=[]\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    for i in list:\n",
    "        if i not in stop_words:\n",
    "            new_list.append(i)\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will now perform these functions to the data set and save it to a new .csv file named \"main2\"\n",
    "\n",
    "# remove urls\n",
    "data['posts'] = data['posts'].str.replace('http\\S+|www.\\S+', ' ', case=False)\n",
    "\n",
    "# applying the functions\n",
    "data['posts'] = data['posts'].apply(cleaning)\n",
    "data['posts'] = data['posts'].apply(tokenize)\n",
    "data['posts'] = data['posts'].apply(lemma)\n",
    "data['posts'] = data['posts'].apply(stopwordremoval)\n",
    "\n",
    "# convert the list representation into a string\n",
    "data['posts']=data['posts'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# save to a csv\n",
    "#data.to_csv(r'/Users/krishnan/desktop/main2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type                                              posts\n",
      "6678  INTJ  ey nice proportion sketch ey nice draw chitin ...\n",
      "1036  INTJ  great day ruined someone unexpectedly come doo...\n"
     ]
    }
   ],
   "source": [
    "print(data.sample(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8675 entries, 0 to 8674\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  8675 non-null   int64 \n",
      " 1   type        8675 non-null   object\n",
      " 2   posts       8674 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 203.4+ KB\n",
      "None\n",
      "Unnamed: 0    0\n",
      "type          0\n",
      "posts         1\n",
      "dtype: int64\n",
      "      Unnamed: 0  type                                              posts\n",
      "5278        5278  ENFP  funny dj hmmm sound right wa wondering thought...\n",
      "2352        2352  INFP  great see dreamed dog loved happy one day play...\n"
     ]
    }
   ],
   "source": [
    "# import the preprocessed data\n",
    "data_prep=pd.read_csv(\"/Users/krishnan/desktop/main2.csv\")\n",
    "\n",
    "# explore data for any issues\n",
    "print(data_prep.info())\n",
    "print(data_prep.isnull().sum())\n",
    "print(data_prep.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3559]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clearly there is a null value in one of the entries in 'posts'\n",
    "data_prep[data_prep['posts'].isnull()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    3559\n",
      "type          INFP\n",
      "posts          NaN\n",
      "Name: 3559, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# inspect\n",
    "print(data_prep.iloc[3559])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will drop it and continue with training and testing\n",
    "data_prep=data_prep.drop(3559, axis=0)\n",
    "data_prep[data_prep['posts'].isnull()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2156\n",
      "Incorrect: 12\n",
      "Accuracy: 0.9944649446494465\n"
     ]
    }
   ],
   "source": [
    "# for this project we will be converting each entry of 'posts' into tf-idf vectors. This ensures a fair \n",
    "# representation of all the tokens and doesnt lead to overrepresentation of a particular token.\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# randomize data\n",
    "randomized_data = data_prep.sample(frac = 1, random_state = 10)\n",
    "\n",
    "# train/test split (75/25)\n",
    "train = randomized_data.sample(6506, random_state = 10)\n",
    "test = randomized_data.sample(2168, random_state = 10)\n",
    "\n",
    "# using train to generate tf-idf vectors\n",
    "train_tfidf = tfidf.fit_transform(train['posts'])\n",
    "\n",
    "# converting test into tf-idf vectors generated by train\n",
    "test_tfidf = tfidf.transform(test['posts'])\n",
    "\n",
    "# train classification lables\n",
    "trainlabels = train['type']\n",
    "\n",
    "# instantiate LinearSVC model and use the train_tfidf to fit the model with trainlabels as the target column\n",
    "lsvc = LinearSVC()\n",
    "lsvc.fit(train_tfidf,trainlabels)\n",
    "\n",
    "# testing\n",
    "test['predictions'] = lsvc.predict(test_tfidf)\n",
    "\n",
    "#check the accuracy of our model with the test data\n",
    "correct = 0\n",
    "total = test.shape[0]\n",
    "\n",
    "for row in test.iterrows():\n",
    "    row = row[1]\n",
    "    if row['type'] == row['predictions']:\n",
    "        correct += 1\n",
    "\n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('Accuracy:', correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained an accuracy of 99.4% for the test data which is very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function that'll take in a message and will classify the user into the predifined categories\n",
    "def conversion(sentence):\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = tokenize(sentence)\n",
    "    sentence = lemma(sentence)\n",
    "    new_list = []\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    for i in sentence:\n",
    "        if i not in stop_words:\n",
    "            new_list.append(i)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    sentence_refined = \" \".join(new_list)\n",
    "    sentence_refined = [sentence_refined]\n",
    "    tfidfed = tfidf.transform(sentence_refined)\n",
    "    category = lsvc.predict(tfidfed)\n",
    "    return(\"You are classed as {}\".format(category[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are classed as ENFP\n"
     ]
    }
   ],
   "source": [
    "print(conversion(\"\"\" yo bro are you coming to my party tonight. it will be super funs. loads of \n",
    "                    people are coming through. let me know if you want in\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are classed as INTJ\n"
     ]
    }
   ],
   "source": [
    "print(conversion(\"\"\"Sorry man , i can't make it tonight. i need to concentrate on my exam tommorrow, need to\n",
    "                    pass my exam otherwise my teacher will report me to the dean\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
